{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4931f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ivi/ilps/personal/kmekonn/projects/envs/ddro_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_url_semantically_rich(url_segments):\n",
    "    generic_terms = {\"index\", \"page\", \"item\", \"view\", \"default\", \"home\"}\n",
    "    descriptive_count = 0\n",
    "    total_segments = len(url_segments)\n",
    "    for segment in url_segments:\n",
    "        if not segment.isnumeric() and segment not in generic_terms and len(segment) > 2:\n",
    "            descriptive_count += 1\n",
    "    return descriptive_count > total_segments / 2\n",
    "\n",
    "def url_docid(input_data, max_docid_len=99, pretrain_model_path=\"t5-base\"):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(pretrain_model_path)\n",
    "    results = {}\n",
    "    skipped_docs = []\n",
    "\n",
    "    for doc_item in tqdm(input_data, desc=\"Processing URL docids\"):\n",
    "        try:\n",
    "            docid = doc_item.get('docid', '').strip().lower()\n",
    "            url = doc_item.get('url', '')\n",
    "            title = doc_item.get('title', '')\n",
    "\n",
    "            if not docid:\n",
    "                skipped_docs.append({\"reason\": \"Missing or empty docid\", \"doc_item\": doc_item})\n",
    "                continue\n",
    "\n",
    "            url = url.strip().lower() if isinstance(url, str) else \"\"\n",
    "            title = title.strip().lower() if isinstance(title, str) else \"\"\n",
    "\n",
    "            if not url and not title:\n",
    "                skipped_docs.append({\"reason\": \"Missing both URL and title\", \"docid\": docid})\n",
    "                continue\n",
    "\n",
    "            url = url.replace(\"http://\", \"\").replace(\"https://\", \"\").replace(\"-\", \" \")\n",
    "            url_segments = [segment for segment in url.split('/') if segment]\n",
    "\n",
    "            domain = url_segments[0] if url_segments else \"\"\n",
    "            reversed_path = \" \".join(reversed(url_segments[1:])) if len(url_segments) > 1 else \"\"\n",
    "\n",
    "            if url_segments and is_url_semantically_rich(url_segments):\n",
    "                final_string = f\"{reversed_path} {domain}\".strip()\n",
    "                source = \"URL\"\n",
    "            elif title and domain:\n",
    "                final_string = f\"{title} {domain}\".strip()\n",
    "                source = \"TITLE\"\n",
    "            elif title:\n",
    "                final_string = title\n",
    "                source = \"TITLE\"\n",
    "            else:\n",
    "                skipped_docs.append({\"reason\": \"Unable to determine final string\", \"docid\": docid})\n",
    "                continue\n",
    "\n",
    "            tokenized_ids = tokenizer(final_string, truncation=True, max_length=max_docid_len).input_ids\n",
    "            tokenized_ids = tokenized_ids[:-1][:max_docid_len] + [1]\n",
    "\n",
    "            results[docid] = {\n",
    "                \"final_string\": final_string,\n",
    "                \"source\": source,\n",
    "                \"token_ids\": tokenized_ids\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            skipped_docs.append({\"reason\": f\"Error: {str(e)}\", \"docid\": doc_item.get('docid', 'unknown')})\n",
    "\n",
    "    if skipped_docs:\n",
    "        print(\"Skipped documents:\")\n",
    "        for skipped in skipped_docs:\n",
    "            print(skipped)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9021d744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URL docids: 100%|██████████| 6/6 [00:00<00:00, 11428.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped documents:\n",
      "{'reason': 'Unable to determine final string', 'docid': '4'}\n",
      "{'reason': 'Missing both URL and title', 'docid': '5'}\n",
      "1:\n",
      "  Used:   URL\n",
      "  String: example.com products item123\n",
      "  Tokens: [677, 5, 287, 494, 2118, 14574, 1]\n",
      "\n",
      "2:\n",
      "  Used:   URL\n",
      "  String: example.com blog how to code\n",
      "  Tokens: [677, 5, 287, 875, 149, 12, 1081, 1]\n",
      "\n",
      "3:\n",
      "  Used:   TITLE\n",
      "  String: fallback title\n",
      "  Tokens: [1590, 1549, 2233, 1]\n",
      "\n",
      "6:\n",
      "  Used:   URL\n",
      "  String: example.com\n",
      "  Tokens: [677, 5, 287, 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo input data\n",
    "data = [\n",
    "    {\"docid\": \"1\", \"url\": \"https://example.com/products/item123\", \"title\": \"Product Page\"},\n",
    "    {\"docid\": \"2\", \"url\": \"https://example.com//blog//how-to-code\", \"title\": \"How to Code\"},\n",
    "    {\"docid\": \"3\", \"url\": None, \"title\": \"Fallback Title\"},\n",
    "    {\"docid\": \"4\", \"url\": \"https://example.com/index\", \"title\": \"\"},\n",
    "    {\"docid\": \"5\", \"url\": \"\", \"title\": \"\"},\n",
    "    {\"docid\": \"6\", \"url\": \"example.com\", \"title\": \"Example Domain\"},\n",
    "]\n",
    "\n",
    "results = url_docid(data)\n",
    "\n",
    "# Display results\n",
    "for docid, info in results.items():\n",
    "    print(f\"{docid}:\")\n",
    "    print(f\"  Used:   {info['source']}\")\n",
    "    print(f\"  String: {info['final_string']}\")\n",
    "    print(f\"  Tokens: {info['token_ids']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba788cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded string for docid [1]: example.com products item123\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Choose a string docid (e.g., \"1\" from your sample data)\n",
    "docid_key = \"1\"\n",
    "tokens = results[docid_key][\"token_ids\"]  # Correct way to access\n",
    "\n",
    "# Decode token IDs\n",
    "decoded_string = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "print(f\"Decoded string for docid [{docid_key}]: {decoded_string}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4931abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URL docids: 100%|██████████| 6/6 [00:00<00:00, 10828.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped documents:\n",
      "{'reason': 'Unable to determine final string', 'docid': '4'}\n",
      "{'reason': 'Missing both URL and title', 'docid': '5'}\n",
      "1:\n",
      "  Used:   URL\n",
      "  String: example.com products item123\n",
      "  Tokens: [677, 5, 287, 494, 2118, 14574, 1]\n",
      "\n",
      "2:\n",
      "  Used:   URL\n",
      "  String: example.com blog how to code\n",
      "  Tokens: [677, 5, 287, 875, 149, 12, 1081, 1]\n",
      "\n",
      "3:\n",
      "  Used:   TITLE\n",
      "  String: fallback title\n",
      "  Tokens: [1590, 1549, 2233, 1]\n",
      "\n",
      "6:\n",
      "  Used:   URL\n",
      "  String: example.com\n",
      "  Tokens: [677, 5, 287, 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "data = [\n",
    "    {\"docid\": \"1\", \"url\": \"https://example.com/products/item123\", \"title\": \"Product Page\"},\n",
    "    {\"docid\": \"2\", \"url\": \"https://example.com//blog//how-to-code\", \"title\": \"How to Code\"},\n",
    "    {\"docid\": \"3\", \"url\": None, \"title\": \"Fallback Title\"},\n",
    "    {\"docid\": \"4\", \"url\": \"https://example.com/index\", \"title\": \"\"},\n",
    "    {\"docid\": \"5\", \"url\": \"\", \"title\": \"\"},\n",
    "    {\"docid\": \"6\", \"url\": \"example.com\", \"title\": \"Example Domain\"},\n",
    "]\n",
    "\n",
    "results = url_docid(data)\n",
    "\n",
    "# Print results\n",
    "for docid, info in results.items():\n",
    "    print(f\"{docid}:\")\n",
    "    print(f\"  Used:   {info['source']}\")\n",
    "    print(f\"  String: {info['final_string']}\")\n",
    "    print(f\"  Tokens: {info['token_ids']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568f0873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URL docids: 100%|██████████| 4/4 [00:00<00:00, 6929.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1:\n",
      "  Used:   TITLE\n",
      "  String: welcome to example 1234\n",
      "  Tokens: [2222, 12, 677, 586, 3710, 1]\n",
      "\n",
      "doc2:\n",
      "  Used:   URL\n",
      "  String: example.com products laptop dell xps\n",
      "  Tokens: [677, 5, 287, 494, 4544, 20, 195, 3, 226, 102, 7, 1]\n",
      "\n",
      "doc3:\n",
      "  Used:   TITLE\n",
      "  String: amazing deals 456\n",
      "  Tokens: [1237, 3694, 314, 4834, 1]\n",
      "\n",
      "doc4:\n",
      "  Used:   URL\n",
      "  String: example.com ai transformers bert\n",
      "  Tokens: [677, 5, 287, 3, 9, 23, 19903, 7, 3, 7041, 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate sample JSONL lines as dicts\n",
    "sample_docs = [\n",
    "    {\"docid\": \"doc1\", \"url\": \"https://example.com/index/1234\", \"title\": \"Welcome to Example\"},\n",
    "    {\"docid\": \"doc2\", \"url\": \"https://example.com/products/laptop/dell-xps\", \"title\": \"Buy Dell XPS Online\"},\n",
    "    {\"docid\": \"doc3\", \"url\": \"https://example.com/page/456\", \"title\": \"Amazing Deals\"},\n",
    "    {\"docid\": \"doc4\", \"url\": \"https://example.com/ai/transformers/bert\", \"title\": \"BERT Model Overview\"},\n",
    "]\n",
    "results = url_docid(sample_docs)\n",
    "\n",
    "# Print results\n",
    "for docid, info in results.items():\n",
    "    print(f\"{docid}:\")\n",
    "    print(f\"  Used:   {info['source']}\")\n",
    "    print(f\"  String: {info['final_string']}\")\n",
    "    print(f\"  Tokens: {info['token_ids']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4de1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
