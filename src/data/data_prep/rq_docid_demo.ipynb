{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca6fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ivi/ilps/personal/kmekonn/projects/envs/ddro_env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from nanopq import PQ\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e749bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_quantization(doc_embeddings, num_levels, cluster_num):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        doc_embeddings: numpy array [num_docs, embedding_dim]\n",
    "        num_levels: how many quantization levels (tokens per docID)\n",
    "        cluster_num: number of clusters per level (codebook size)\n",
    "    Returns:\n",
    "        docid_to_rqcode: list of token lists, one per doc\n",
    "    \"\"\"\n",
    "    residuals = doc_embeddings.copy()\n",
    "    codebooks = []\n",
    "    docid_to_rqcode = [[] for _ in range(len(doc_embeddings))]\n",
    "\n",
    "    for level in range(num_levels):\n",
    "        print(f\"Training level {level+1}/{num_levels}...\")\n",
    "\n",
    "        # Step 1: Train codebook using KMeans on the current residuals\n",
    "        kmeans = KMeans(n_clusters=cluster_num, random_state=42)\n",
    "        kmeans.fit(residuals)\n",
    "        codebook = kmeans.cluster_centers_\n",
    "        codebooks.append(codebook)\n",
    "\n",
    "        # Step 2: For each doc, assign nearest codeword and compute new residual\n",
    "        new_residuals = []\n",
    "\n",
    "        for doc_idx, doc_vec in enumerate(residuals):\n",
    "            # Compute distances to all centroids in the codebook\n",
    "            distances = np.linalg.norm(codebook - doc_vec, axis=1)\n",
    "            nearest_codeword_idx = np.argmin(distances)\n",
    "            nearest_codeword = codebook[nearest_codeword_idx]\n",
    "\n",
    "            # Save token index for this level\n",
    "            docid_to_rqcode[doc_idx].append(int(nearest_codeword_idx))\n",
    "\n",
    "            # Compute residual for next level\n",
    "            residual = doc_vec - nearest_codeword\n",
    "            new_residuals.append(residual)\n",
    "\n",
    "        residuals = np.vstack(new_residuals)\n",
    "\n",
    "    return docid_to_rqcode, codebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d5ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define text passages\n",
    "passages = {\n",
    "    \"doc1\": \"Artificial intelligence is transforming industries.\",\n",
    "    \"doc2\": \"Machine learning helps computers learn from data.\",\n",
    "    \"doc3\": \"Quantum computing is a new paradigm.\",\n",
    "    \"doc4\": \"The future of AI includes ethical challenges.\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "359b3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load GTR-T5-Base sentence encoder\n",
    "model = SentenceTransformer(\"sentence-transformers/gtr-t5-base\")\n",
    "docids = list(passages.keys())\n",
    "texts = list(passages.values())\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50975f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 768),\n",
       " dtype('float32'),\n",
       " array([[ 0.00273099, -0.0302944 ,  0.05884   , ...,  0.03470925,\n",
       "         -0.01520736,  0.03500362],\n",
       "        [-0.02251787, -0.05412124,  0.05713512, ...,  0.00543885,\n",
       "          0.00965429, -0.01286693],\n",
       "        [ 0.01178704, -0.01564313,  0.01263544, ..., -0.0136919 ,\n",
       "         -0.00466294,  0.01595624],\n",
       "        [ 0.01346409, -0.00766769,  0.01716973, ..., -0.00420209,\n",
       "         -0.02281265,  0.05193746]], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape, embeddings.dtype, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ef9dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training level 1/3...\n",
      "Training level 2/3...\n",
      "Training level 3/3...\n",
      "doc1: Semantic ID = [1, 0, 0]\n",
      "doc2: Semantic ID = [0, 0, 0]\n",
      "doc3: Semantic ID = [1, 0, 1]\n",
      "doc4: Semantic ID = [1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "num_levels = 3  # Let's keep it small for this example\n",
    "cluster_num = 2 # Tiny number since we only have 4 documents  # 256 Typical size for codebooks\n",
    "\n",
    "rq_codes, codebooks = residual_quantization(embeddings, num_levels, cluster_num)\n",
    "\n",
    "# Show Semantic IDs for each doc\n",
    "for docid, rq_code in zip(docids, rq_codes):\n",
    "    print(f\"{docid}: Semantic ID = {rq_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14962f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized RQ DocIDs:\n",
      "doc1\t32129,32384,32640\n",
      "doc2\t32128,32384,32640\n",
      "doc3\t32129,32384,32641\n",
      "doc4\t32129,32385,32640\n"
     ]
    }
   ],
   "source": [
    "mock_vocab_size = 32128\n",
    "\n",
    "print(\"Quantized RQ DocIDs:\")\n",
    "encoded_ids = []\n",
    "\n",
    "for idx, code in enumerate(rq_codes):\n",
    "    # Offset each token by (level * 256) to avoid collisions between levels\n",
    "    new_doc_code = [int(x) + i * 256 for i, x in enumerate(code)]\n",
    "    # Shift into the vocabulary space (avoid overlap with normal tokens)\n",
    "    encoded = ','.join(str(x + mock_vocab_size) for x in new_doc_code)\n",
    "    encoded_ids.append(encoded)\n",
    "    print(f\"{docids[idx]}\\t{encoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3583c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded IDs and their closest texts:\n",
      "Decoded text for doc 1: Artificial intelligence is transforming industries.\n",
      "Similarity scores for doc 1:\n",
      "  Text 1: Artificial intelligence is transforming industries. (Similarity: 0.9545)\n",
      "  Text 2: Machine learning helps computers learn from data. (Similarity: 0.6810)\n",
      "  Text 3: Quantum computing is a new paradigm. (Similarity: 0.7936)\n",
      "  Text 4: The future of AI includes ethical challenges. (Similarity: 0.7724)\n",
      "Decoded text for doc 2: Machine learning helps computers learn from data.\n",
      "Similarity scores for doc 2:\n",
      "  Text 1: Artificial intelligence is transforming industries. (Similarity: 0.7010)\n",
      "  Text 2: Machine learning helps computers learn from data. (Similarity: 0.9828)\n",
      "  Text 3: Quantum computing is a new paradigm. (Similarity: 0.5172)\n",
      "  Text 4: The future of AI includes ethical challenges. (Similarity: 0.4770)\n",
      "Decoded text for doc 3: Quantum computing is a new paradigm.\n",
      "Similarity scores for doc 3:\n",
      "  Text 1: Artificial intelligence is transforming industries. (Similarity: 0.6038)\n",
      "  Text 2: Machine learning helps computers learn from data. (Similarity: 0.5617)\n",
      "  Text 3: Quantum computing is a new paradigm. (Similarity: 1.0000)\n",
      "  Text 4: The future of AI includes ethical challenges. (Similarity: 0.5799)\n",
      "Decoded text for doc 4: The future of AI includes ethical challenges.\n",
      "Similarity scores for doc 4:\n",
      "  Text 1: Artificial intelligence is transforming industries. (Similarity: 0.6858)\n",
      "  Text 2: Machine learning helps computers learn from data. (Similarity: 0.5446)\n",
      "  Text 3: Quantum computing is a new paradigm. (Similarity: 0.4875)\n",
      "  Text 4: The future of AI includes ethical challenges. (Similarity: 0.9893)\n"
     ]
    }
   ],
   "source": [
    "reconstructed_embeddings = []\n",
    "\n",
    "for code in rq_codes:\n",
    "    # Start with zero vector\n",
    "    recon = np.zeros_like(embeddings[0])\n",
    "    for level, token in enumerate(code):\n",
    "        recon += codebooks[level][token]  # Sum codewords from each level\n",
    "    reconstructed_embeddings.append(recon)\n",
    "\n",
    "reconstructed_embeddings = np.vstack(reconstructed_embeddings)\n",
    "\n",
    "# Normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "embeddings_norm = normalize(embeddings, axis=1)\n",
    "reconstructed_embeddings_norm = normalize(reconstructed_embeddings, axis=1)\n",
    "\n",
    "# Compute cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(reconstructed_embeddings_norm, embeddings_norm)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDecoded IDs and their closest texts:\")\n",
    "for i, sim in enumerate(similarities):\n",
    "    closest_idx = np.argmax(sim)\n",
    "    print(f\"Decoded text for doc {i + 1}: {texts[closest_idx]}\")\n",
    "    print(f\"Similarity scores for doc {i + 1}:\")\n",
    "    for j, score in enumerate(sim):\n",
    "        print(f\"  Text {j + 1}: {texts[j]} (Similarity: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca419a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
