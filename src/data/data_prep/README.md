# ğŸ§± DDRO Data Preparation & Instance Generation

This directory contains unified scripts for preparing and transforming data for **Direct Document Relevance Optimization (DDRO)**. It covers:

* Preprocessing MS MARCO and Natural Questions datasets
* Generating dense document embeddings
* DocID encoding (URL, PQ, Atomic)
* Creating training and evaluation instances

---


## ğŸ“ Available Scripts â€” `src/data/data_prep/`

This folder contains all core scripts for preprocessing datasets, generating document embeddings, encoding docids, and preparing training/evaluation instances for DDRO.

---

### ğŸ§¼ Dataset Preprocessing

Scripts for transforming and sampling datasets:

```bash
data_prep/
â”œâ”€â”€ convert_tsv_to_json_array.py        # Convert MS MARCO .tsv to flat JSONL format
â”œâ”€â”€ sample_top300k_msmarco_documents.py # Sample top or random 300K MS MARCO documents
â”œâ”€â”€ negative_sampling.py                # BM25-based hard negative mining
â”œâ”€â”€ doc2query_query_generator.py        # Generate pseudoqueries using doc2query-T5
â”œâ”€â”€ generate_doc_embeddings.py          # Generate GTR-T5 dense embeddings
```

---

### ğŸ§ª Natural Questions (NQ) Utilities

Located under the `nq/` subdirectory:

```bash
nq/
â”œâ”€â”€ process_nq_dataset.py               # Clean, flatten and merge original NQ files
â”œâ”€â”€ convert_nq_to_msmarco_format.py     # Convert NQ format â†’ MS MARCO query-passage format
â”œâ”€â”€ create_nq_triples.py                # Create BM25-based NQ training triples
```

---

### ğŸ“¦ Instance Generation

Scripts for preparing model training and evaluation input instances:

```bash
data_prep/
â”œâ”€â”€ generate_encoded_docids.py          # Encode documents into docids (PQ, URL, etc.)
â”œâ”€â”€ generate_train_data_wrapper.py      # Wrapper to generate all training instances
â”œâ”€â”€ generate_train_instances.py         # Create pretrain, pseudoquery, and finetune inputs
â”œâ”€â”€ generate_eval_data_wrapper.py       # Wrapper for evaluation data preparation
â”œâ”€â”€ generate_eval_instances.py          # Format eval data for testing SFT/DDRO
```

---

### ğŸ““ Demos & Explorations

Notebook demos for docid formats and encodings:

```bash
data_prep/
â”œâ”€â”€ pq_docid_demo.ipynb                 # Visualize PQ-based docid encoding
â”œâ”€â”€ rq_docid_demo.ipynb                 # Visualize ranking-quality based docid
â”œâ”€â”€ url_title_docid_demo.ipynb          # URL+title docid construction example
```

---

### ğŸ—‚ï¸ This README

```bash
data_prep/
â””â”€â”€ README.md                           # Youâ€™re here!
```


## ğŸ“Š Data Preparation

DDRO supports both **MS MARCO** and **Natural Questions (NQ)** benchmarks.

### âœ… MS MARCO: Sample Top-300K Subset

```bash
sbatch src/scripts/preprocess/sample_top_docs.sh
```

ğŸ“Œ Generates: `resources/datasets/processed/msmarco-docs-sents.top.300k.json.gz`
(JSONL format, sentence-tokenized, ranked by qrels frequency)

---

## ğŸ”¢ DocID Representations

You can either **generate** document ID (`docid`) representations locally or **download** pre-computed ones from ğŸ¤— [Hugging Face](https://huggingface.co/collections/kiyam/ddro-generative-document-retrieval-680f63f2e9a72033598461c5).

Place downloaded files in:

```bash
resources/datasets/processed/msmarco-data/encoded_docid/
```

#### ğŸ“„ Example: `url_docid` format

```text
[d108472] 594,858,7,17,4624,5,287,1
[d1842]   3,89,9,1824,3105,4440,...,1677,1
```

---

### ğŸ› ï¸ Generating DocID Representations Locally

To generate PQ-based docids (**used in this project**):

#### 1ï¸âƒ£ Generate T5 Document Embeddings

âš ï¸ **Attention:** Ensure the script sets the dataset to `"msmarco"`:

```bash
sbatch src/scripts/preprocess/generate_doc_embeddings.sh
```

#### 2ï¸âƒ£ Encode Document IDs

Use the script below to generate `pq` docids (other types are also supported):

```bash
sbatch src/scripts/preprocess/generate_encoded_ids.sh
```

âš ï¸  Pass --encoding pq (default) or --encoding url to specify the format.

---

#### ğŸ“„ Example: `pq_docid` format

```text
[d108472] 32211,32518,32782,33144,33382,...
[d1842]   32177,32471,32844,33053,33163,...
```
---

## ğŸ›  Training Instance Generation

To train the **Phase 1: Supervised Fine-Tuning (SFT)** model, we generate three types of `input â†’ docid` training instances. These follow a **curriculum-based progression** aligned with document relevance modeling.

---

### ğŸ¯ SFT Training Stages

| Stage                     | Input â†’ Target        | Purpose                             |
| ------------------------- | --------------------- | ----------------------------------- |
| **1. Pretraining**        | `doc â†’ docid`         | General content understanding       |
| **2. Search Pretraining** | `pseudoquery â†’ docid` | Learn retrieval-like query behavior |
| **3. Finetuning**         | `query â†’ docid`       | Supervised learning from qrels      |

---

### âœï¸ Pseudo Query Generation (docTTTTTquery)

To enable search pretraining, generate pseudo queries from raw documents using a docT5query model, producing 10 queries per document.

#### ğŸ§ª Finetune docTTTTTquery on NQ or MS MARCO:

```bash
bash scripts/run_finetune_docTTTTTquery.sh
```

#### ğŸ›  Generate queries:

```bash
python src/data/data_prep/doc2query_query_generator.py
```

#### ğŸ“¥ Or download from Hugging Face:

**[DDRO ğŸ¤— Collection](https://huggingface.co/collections/kiyam/ddro-generative-document-retrieval-680f63f2e9a72033598461c5)**

Place the downloaded queries under:

```
ddro/resources/datasets/processed/msmarco-data/msmarco_pseudo_query_10.txt  
ddro/resources/datasets/processed/nq-data/nq_pseudo_query_10.txt
```

Expected format (`[docid] <TAB> pseudo_query`):

```
[d301595]	what age should a child know what they like to eat  
[d301595]	what is the average age for a child to be independent  
...
```

---

### âœ… Step 1: Generate Raw Supervision Signals

Generate inputs for each supervision type:

```bash
sbatch src/scripts/preprocess/gen_t5_train_data.sh
```

Outputs:

* `passage.jsonl` â€” raw document text
* `sampled_terms.jsonl` â€” sampled key phrases
* `fake_query.jsonl` â€” pseudo queries
* `query.jsonl` â€” real queries from qrels

---

### âš™ï¸ Step 2: Merge into Curriculum Format

Format into `input â†’ docid` pairs for training:

```bash
sbatch ddro/src/scripts/preprocess/generate_t5_eval_and_train_data.sh
```

Produces:

* `pretrain.t5_128_10.json`
* `search_pretrain.t5_128_10.json`
* `finetune.t5_128_1.json`

Each corresponds to a curriculum stage.


| Mode (`--cur_data`) | Input File         | Output File                      | Description                   |
| ------------------- | ------------------ | -------------------------------- | ----------------------------- |
| `general_pretrain`  | `passage.jsonl + sampled_terms.jsonl + enhanced_docid.jsonl`    | `pretrain.t5_128_10.json`        | Raw doc â†’ docid               |
| `search_pretrain`   | `fake_query.jsonl` | `search_pretrain.t5_128_10.json` | Pseudo query â†’ docid          |
| `finetune`          | `query.jsonl`      | `finetune.t5_128_1.json`         | Real query + qrel doc â†’ docid |

---

### ğŸš€ Generating Training + Evaluation Data

Use the following entry-point scripts to generate both **training** and **evaluation** instances for MS MARCO and NQ. These scripts internally invoke the wrapper and handle all required paths and configurations.


```bash
bash src/scripts/preprocess/generate_nq_eval_and_train_data.sh
```


> These scripts automatically generate data for all three stages (`general_pretrain`, `search_pretrain`, and `finetune`) as well as the corresponding evaluation files.

---

## ğŸ§² Contrastive Triplets (For Phase 2: DDRO)

After training the SFT model, we move to **Phase 2: Direct Document Relevance Optimization (DDRO)**, which fine-tunes the model with a **pairwise ranking objective** using contrastive triplets.

Each triplet contains:

* A query
* A **positive** document ID
* One or more **negative** document IDs

---

### ğŸ“¦ Triplet Generation

To create these contrastive training triplets:

#### ğŸ”¹ Natural Questions (NQ)

```bash
python ddro/src/data/dataprep/create_nq_triples.py
```

---

#### ğŸ”¹ MS MARCO

1. Download the official top-100 BM25 retrievals:
   ğŸ“¥ [msmarco-doctrain-top100.gz](https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-doctrain-top100.gz)

2. Run the triplet generation script:

```bash
python src/data/dataprep/generate_msmarco_triples.py
```

You may also adapt the script to use your **own BM25 ranking outputs**.
---

### âœ… Natural Questions (NQ)


To prepare the **Natural Questions (NQ)** dataset in MS MARCO-style format and generate training-ready encodings, follow these steps:

#### 1. Preprocess and convert the dataset:

```bash
bash scripts/preprocess/preprocess_nq_dataset.sh               # Cleans and merges NQ
bash scripts/preprocess/convert_nq_to_msmarco_format.sh        # Converts to MS MARCO-style format
```

after haing m,smarco style dataset follow the sem step as above replace data patrh and atatype  as 'nq'

#### 2. Generate document IDs and training instances:

> âœ… Make sure the dataset argument in each script is set to `"nq"`.

```bash
sbatch src/scripts/preprocess/generate_doc_embeddings.sh  # Step 1: Compute document embeddings (required for PQ assignment)
bash scripts/preprocess/generate_encoded_ids.sh           # Step 2: Generate and save encoded doc IDs (URL, PQ, etc.)
```
> only pq and url IDs reported on our paper 

---

Maintained with â¤ï¸ by the DDRO authors.
