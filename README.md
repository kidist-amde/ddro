# DDRO: Direct Document Relevance Optimization for Generative IR

This repository contains the official implementation for our SIGIR 2025 paper:  
**"Lightweight and Direct Document Relevance Optimization for Generative IR (DDRO)"**.

We propose a lightweight and effective optimization framework for generative IR that directly aligns document ID generation with document-level relevance using a pairwise ranking objective.

---

## ğŸ“ Project Structure

```bash
ddro/
â”œâ”€â”€ data/                # Data downloading, preprocessing, and docid instance generation
â”œâ”€â”€ pretrain/            # DDRO model training and evaluation logic (incl. DPO)
â”œâ”€â”€ scripts/             # Entry-point shell scripts for SFT, DPO, BM25, and preprocessing
â”œâ”€â”€ utils/               # Core utilities (tokenization, trie, metrics, trainers)
â”œâ”€â”€ ddro.yml             # Conda environment (for training DDRO)
â”œâ”€â”€ pyserini.yml         # Conda environment (for BM25 retrieval with Pyserini)
â”œâ”€â”€ README.md            # You're here!
â””â”€â”€ requirements.txt     # Additional Python dependencies
```

Each subdirectory includes a README where needed.

---

## ğŸ› ï¸ Setup & Dependencies

1. Clone the repo and install dependencies:
   ```bash
   git clone https://github.com/kidist-amde/DDRO-Direct-Document-Relevance-Optimization.git
   conda env create -f ddro.yml
   conda activate ddro
   ```

2. Download the datasets and T5 model:
   ```bash
   bash data/download/download_msmarco_datasets.sh
   bash data/download/download_nq_datasets.sh
   python data/download/download_t5_model.py
   ```

3. Place them into the following structure:
   ```
   resources/
   â”œâ”€â”€ datasets/
   â”‚   â”œâ”€â”€ msmarco-data/
   â”‚   â””â”€â”€ nq-data/
   â””â”€â”€ transformer_models/
       â””â”€â”€ t5-base/
   ```

---

## ğŸ“¦ Data Preparation

### âœ… For Natural Questions

Run the full NQ preparation pipeline:
```bash
bash scripts/preprocess_nq_dataset.sh               # Cleans and merges NQ
bash scripts/convert_nq_to_msmarco_format.sh        # Converts to MS MARCO style
bash scripts/compute_nq_t5_embeddings.sh            # T5-based embeddings
bash scripts/generate_nq_encoded_ids.sh             # Encode docids (URL, PQ, Atomic)
bash scripts/generate_nq_eval_and_train_data.sh     # Eval/train file generation
```

### âœ… For MS MARCO

```bash
bash scripts/preprocess/sample_top_docs.sh
sbatch scripts/generate_msmarco_t5_embeddings.sh
sbatch scripts/generate_msmarco_encoded_ids.sh
```

---

## ğŸ” Training Pipeline

The DDRO training proceeds in 3 stages. Use:

```bash
python utils/run_training_pipeline.py --encoding pq
```

It runs:
- Stage 1: Pretraining on document contents
- Stage 2: Pretraining on doc2query outputs
- Stage 3: Finetuning on BM25 negatives or labeled qrels

---

## ğŸ” BM25 Retrieval Setup (Pyserini)

```bash
conda env create -f pyserini.yml
conda activate pyserini
pip install -r pyserini.txt
```

Run indexing and retrieval:
```bash
bash scripts/bm25/run_bm25_retrieval_nq.sh
bash scripts/bm25/run_bm25_retrieval_msmarco.sh
```

---

##  DPO Training

See `scripts/dpo/` for lightweight pairwise fine-tuning using [HF's `DPOTrainer`](https://github.com/huggingface/trl).  
Example:
```bash
bash scripts/dpo/run_dpo_training.sh
bash scripts/dpo/run_test_dpo.sh
```

---


Evaluation logs and per-query metrics are saved under:
```
logs/
outputs/
```

---

## ğŸ“š Datasets Used

- [MS MARCO Document Ranking](https://microsoft.github.io/msmarco/)
- [Natural Questions](https://ai.google.com/research/NaturalQuestions)

---

## ğŸ™ Acknowledgments

We gratefully acknowledge these open-source projects:

- [ULTRON](https://github.com/smallporridge/WebUltron)
- [HuggingFace TRL](https://github.com/huggingface/trl)
- [NCI](https://github.com/solidsea98/Neural-Corpus-Indexer-NCI)
- [docTTTTTquery](https://github.com/castorini/docTTTTTquery)

---

## ğŸ“„ License

Licensed under the [Apache 2.0 License](LICENSE).

---

## ğŸ“Œ Citation

```bibtex
@inproceedings{kidiy2025ddro,
  title = {Lightweight and Direct Document Relevance Optimization for Generative IR},
  author = {Author Name(s)},
  booktitle = {SIGIR},
  year = {2025}
}
```

---

## Contact

For questions, please open an [issue](https://github.com/kidist-amde/DDRO-Direct-Document-Relevance-Optimization/issues).


